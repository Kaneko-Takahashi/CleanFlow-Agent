# 要件定義書 - CleanFlow Agent

## システム概要

### プロダクト名
CleanFlow Agent

### 目的
CSVデータをアップロードし、AIエージェントが自動的にデータプロファイリングと前処理プランの生成・実行を行うツール。ユーザーは分類・回帰・クラスタリングなどのタスク種別とターゲット列を指定するだけで、適切な前処理ステップ（欠損補完・エンコード・スケーリングなど）を自動生成し、実行できる。

### 想定ユーザー
- データサイエンティスト
- 機械学習エンジニア
- データ分析初心者
- データ前処理の自動化を求める開発者

## 対応データ

- **形式**: CSV（カンマ区切り）
- **構造**: 
  - 行 = サンプル（データポイント）
  - 列 = 特徴量（特徴変数）
- **制約**: 
  - ファイルサイズ上限（初期は100MB程度を想定）
  - 文字エンコーディング: UTF-8推奨（Shift-JIS等も対応可能）

## 機能要件

### 認証機能

#### ユーザー登録
- メールアドレス + パスワードで新規ユーザー登録
- メールアドレスの重複チェック
- パスワード強度チェック（最低8文字以上など）
- 登録成功時に自動ログイン（オプション）

#### ログイン
- メールアドレス + パスワードでログイン
- ログイン成功時にJWTアクセストークンを発行
- トークンはAuthorizationヘッダで送信

#### ログアウト
- トークンの無効化（MVPではクライアント側で削除のみ）

### データセット管理機能

#### CSVアップロード
- ログイン中ユーザーのみアップロード可能
- ファイル選択・アップロード
- アップロード時にデータセット名を指定（デフォルトはファイル名）
- サーバー側でファイルを保存（ローカルストレージまたはDB）

#### データセット一覧
- ログイン中ユーザーが所有するデータセットのみ表示
- 表示項目：
  - データセット名
  - 行数
  - 列数
  - 作成日時
  - 詳細ボタン

#### データセット削除
- 自分のデータセットのみ削除可能
- 削除時にファイルとDBレコードを削除

### データプロファイリング機能

#### プロファイル生成
- データセットアップロード時に自動生成
- 各列の情報：
  - データ型（int, float, object, datetime等）
  - 欠損値の数・欠損率
  - ユニーク値の数
  - 基本統計量（平均、中央値、標準偏差、最小値、最大値など）
  - カテゴリ変数の場合は頻度分布

#### プロファイル表示
- データセット詳細画面で表示
- テーブル形式で各列のプロファイルを一覧表示

### 前処理プラン生成機能

#### タスク種別指定
- 分類（Classification）
- 回帰（Regression）
- クラスタリング（Clustering）

#### ターゲット列指定
- タスク種別に応じてターゲット列を選択
- 分類・回帰の場合は必須、クラスタリングの場合は不要

#### AIエージェントによるプラン生成
- LLM（OpenAI GPT-4等）を使用
- データプロファイルとタスク種別・ターゲット列を入力
- 前処理ステップをJSON形式で生成：
  - ステップの順序（order）
  - ステップ名（name）
  - 説明（description）
  - 実行コード（code_snippet: pandas/scikit-learn）

#### 生成される前処理ステップの例
- 欠損値補完（平均値、中央値、最頻値、前後の値など）
- カテゴリ変数のエンコード（One-Hot Encoding、Label Encoding）
- 数値変数のスケーリング（StandardScaler、MinMaxScaler）
- 外れ値の処理
- 特徴量選択
- データ分割（train/test）

### プラン実行機能

#### プラン実行
- 生成された前処理プランを実行
- pandas/scikit-learnのコードを安全に実行
- 実行ログを記録

#### Before/Afterサマリ表示
- 実行前のデータ形状・統計量
- 実行後のデータ形状・統計量
- 各ステップの実行結果（成功/失敗、処理時間など）

### 前処理レシピの保存・再利用

#### プラン保存
- 生成されたプランをデータベースに保存
- プラン名を付与可能

#### プラン一覧
- ユーザーが作成したプランの一覧表示
- プラン名、作成日時、関連データセット名を表示

#### プラン再利用
- 保存されたプランを他のデータセットに適用（MVPでは簡易版）

## 非機能要件

### セキュリティ

#### 認証の安全性
- パスワードはハッシュ化して保存（bcrypt等を使用）
- JWTのシークレットキーは環境変数で管理
- トークンの有効期限設定（例: 24時間）
- HTTPS通信を推奨（本番環境）

#### データ分離
- ユーザー間でデータが混在しないよう、user_idで厳密に分離
- ファイルアクセス権限の適切な設定

### パフォーマンス

#### 応答時間
- ログイン: 1秒以内
- CSVアップロード: ファイルサイズに依存（10MBで5秒以内）
- プロファイル生成: データセットサイズに依存（10万行で10秒以内）
- プラン生成: LLM API呼び出しを含むため30秒以内
- プラン実行: データセットサイズとステップ数に依存

#### スケーラビリティ
- 初期は単一サーバーで動作
- 将来的に複数インスタンス対応可能な設計

### 使用技術

#### バックエンド
- Python 3.10以上
- FastAPI
- SQLite（開発時）、PostgreSQL（本番環境想定）
- pandas / scikit-learn（データ処理）
- JWT認証（python-jose、passlib）

#### LLM連携
- OpenAI API（GPT-4等）
- または他のLLM API（Claude、Gemini等）

### ログ・監視

#### ログ出力
- アプリケーションログ（INFO、WARNING、ERROR）
- アクセスログ（API呼び出し）
- エラーログ（スタックトレース含む）

#### 監視項目
- API応答時間
- エラー率
- LLM API呼び出し回数・コスト

## 将来拡張

### フロントエンド連携
- Supabase / Vercelと連携したフロントエンド開発
- React / Next.js等を使用したSPA
- リアルタイム更新（WebSocket等）

### マルチテナントSaaS化
- 組織・チーム機能
- データセットの共有機能
- プランの公開・共有
- サブスクリプションモデル

### 機能拡張
- より高度な前処理ステップ（特徴量エンジニアリング、次元削減等）
- 複数のLLMプロバイダー対応
- プランのバージョン管理
- 実行履歴の詳細分析
- データ可視化機能
- バッチ処理機能

